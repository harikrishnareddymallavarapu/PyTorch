{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch-MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPG161ve+GkbnMD6lFqLWUV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harikrishnareddymallavarapu/PyTorch/blob/main/PyTorch_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp9yqfKZgqNh"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure.format ='retina'\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import helper\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL9iBeL4hPOO"
      },
      "source": [
        "# MNIST data set \n",
        "- Each image is converted to 28*28 pixel value\n",
        "-transforms.Compose just clubs all the transforms provided to it. So, all the transforms in the transforms.Compose are applied to the input one by one.\n",
        "-Normalize does the following for each channel:\n",
        "\n",
        "  image = (image - mean) / std\n",
        "\n",
        "  The parameters mean, std are passed as 0.5, 0.5 in your case. This will normalize the image in the range [-1,1]. For example, the minimum value 0 will be converted to (0-0.5)/0.5=-1, the maximum value of 1 will be converted to (1-0.5)/0.5=1.\n",
        "\n",
        "  if you would like to get your image back in [0,1] range, you could use,\n",
        "\n",
        "  image = ((image * std) + mean)\n",
        "\n",
        "  Normalization helps get data within a range and reduces the skewness which helps learn faster and better\n",
        "\n",
        "  Color images have three channels (red, green, blue), therefore you need three parameters to normalize each channel. The first tuple (0.5, 0.5, 0.5) is the mean for all three channels and the second (0.5, 0.5, 0.5) is the standard deviation for all three channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxsfaF0zlMeD"
      },
      "source": [
        "torchvision.transforms.Normalize(mean, std, inplace=False)\n",
        "\n",
        "- Normalize a tensor image with mean and standard deviation. Given mean: (mean[1],...,mean[n]) and std: (std[1],..,std[n]) for n channels, this transform will normalize each channel of the input torch.*Tensor i.e., output[channel] = (input[channel] - mean[channel]) / std[channel]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SWhkRH_gyUn"
      },
      "source": [
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-R1Vp3siVs7"
      },
      "source": [
        "#Defining the transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,),(0.5,)),])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3kLdZ9WlgpM"
      },
      "source": [
        "#Download and load the training data\n",
        "trainset = datasets.MNIST('MNIST-data/', download=True, train=True, transform=transform)\n",
        "trainLoader = torch.utils.data.DataLoader(trainset, shuffle=True,batch_size=64)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y-crhbAmZ7u",
        "outputId": "e589903d-1d0e-49b7-9596-fcf6dca4dc4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "trainLoader"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f9fd977d908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ramyxnGzm8RK",
        "outputId": "0f875a25-d121-4bab-f897-a3003c9ae59d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "dataiter = iter(trainLoader)\n",
        "images, labels = dataiter.next()\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBWqH177oFEz",
        "outputId": "c62f240a-d260-4a0a-fed7-e41abfa52b4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.imshow(images[1].numpy().squeeze(),cmap = 'Greys_r')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9fd9281c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYjkKu_1obUx"
      },
      "source": [
        "#Exercise \n",
        "- Flatten the batch of images,. Then build a multi layer network with 784 inputs and 256 hidden units and 10 output units using random tensors for wieghts and biases. Sigmoid layer as activation function for hidden layers and softmax in the output (this follows a probability distribution) \n",
        "- 10 outpus to identify the 10 digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoEhiJ9KpBkc"
      },
      "source": [
        "#Flatten the images\n",
        "images[0].view(784,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0uN39R4rOv9"
      },
      "source": [
        "#Flattening the input images\n",
        "input = images.view(784,-1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qk2ushirivA"
      },
      "source": [
        "#Creating weights and biases\n",
        "#Layer 1 \n",
        "weightsLayer1 = torch.randn(784,256)\n",
        "biasLayer1 = torch.randn(256)\n",
        "\n",
        "#Layer 2 \n",
        "weightsLayer2 = torch.randn(256,10)\n",
        "biasLayer2 = torch.randn(10)"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}